{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9011f4",
   "metadata": {},
   "source": [
    "# CORD-19 â€” Analysis Notebook\n",
    "\n",
    "This notebook implements the assignment: Part 1 (loading & exploration), Part 2 (cleaning), Part 3 (analysis & exports), Part 4 (Streamlit hooks), and Part 5 (short reflection). It prefers a small `metadata_sample.csv` for fast iteration but can create one from the full `metadata.csv` if present. Outputs are written to an `outputs/` folder at the repo root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6da05c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata_sample.csv\n",
      "Loaded shape: (50000, 19)\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Robust loading of the sample or original metadata file\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def find_or_create_sample(nrows=50000):\n",
    "    candidates = [\n",
    "        Path('metadata_sample.csv'),\n",
    "        Path.cwd() / 'metadata_sample.csv',\n",
    "        Path.cwd().parent / 'metadata_sample.csv',\n",
    "        Path('metadata.csv'),\n",
    "        Path.cwd() / 'data' / 'metadata.csv',\n",
    "        Path.cwd().parent / 'metadata.csv',\n",
    "        Path.cwd().parent / 'data' / 'metadata.csv',\n",
    "    ]\n",
    "    # try direct sample locations first\n",
    "    for p in candidates[:3]:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # if no sample, try to make one from metadata.csv\n",
    "    for o in candidates[3:]:\n",
    "        if o.exists():\n",
    "            try:\n",
    "                print('Creating sample from', o)\n",
    "                df = pd.read_csv(o, low_memory=False, nrows=nrows)\n",
    "                out = Path.cwd() / 'metadata_sample.csv'\n",
    "                df.to_csv(out, index=False)\n",
    "                return out\n",
    "            except Exception as e:\n",
    "                print('Failed to create sample:', e)\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "sample_path = find_or_create_sample()\n",
    "if sample_path is None:\n",
    "    raise FileNotFoundError('No metadata_sample.csv or metadata.csv found in expected locations. Place dataset in project root or data/ folder.')\n",
    "\n",
    "print('Loading', sample_path)\n",
    "df = pd.read_csv(sample_path, low_memory=False)\n",
    "print('Loaded shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64302d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['cord_uid', 'sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id', 'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id', 'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'url', 's2_id']\n",
      "\n",
      "Missing values (top 10 cols):\n",
      "who_covidence_id    50000\n",
      "arxiv_id            50000\n",
      "s2_id               50000\n",
      "mag_id              50000\n",
      "pubmed_id           16876\n",
      "pmc_json_files      14542\n",
      "abstract            13997\n",
      "sha                 10257\n",
      "pdf_json_files      10257\n",
      "authors              4630\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "object     15\n",
      "float64     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows:\n",
      "   cord_uid                                       sha source_x  \\\n",
      "0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb      PMC   \n",
      "1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d      PMC   \n",
      "2  ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927      PMC   \n",
      "3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n",
      "4  9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32      PMC   \n",
      "\n",
      "                                               title                    doi  \\\n",
      "0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n",
      "1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n",
      "2    Surfactant protein-D and pulmonary host defense           10.1186/rr19   \n",
      "3               Role of endothelin-1 in lung disease           10.1186/rr44   \n",
      "4  Gene expression in epithelial cells in respons...           10.1186/rr61   \n",
      "\n",
      "      pmcid pubmed_id license  \\\n",
      "0  PMC35282  11472636   no-cc   \n",
      "1  PMC59543  11667967   no-cc   \n",
      "2  PMC59549  11667972   no-cc   \n",
      "3  PMC59574  11686871   no-cc   \n",
      "4  PMC59580  11686888   no-cc   \n",
      "\n",
      "                                            abstract publish_time  \\\n",
      "0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
      "1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
      "2  Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
      "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
      "4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
      "\n",
      "                                             authors         journal  mag_id  \\\n",
      "0                Madani, Tariq A; Al-Ghamdi, Aisha A  BMC Infect Dis     NaN   \n",
      "1  Vliet, Albert van der; Eiserich, Jason P; Cros...      Respir Res     NaN   \n",
      "2                                    Crouch, Erika C      Respir Res     NaN   \n",
      "3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M      Respir Res     NaN   \n",
      "4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...      Respir Res     NaN   \n",
      "\n",
      "   who_covidence_id  arxiv_id  \\\n",
      "0               NaN       NaN   \n",
      "1               NaN       NaN   \n",
      "2               NaN       NaN   \n",
      "3               NaN       NaN   \n",
      "4               NaN       NaN   \n",
      "\n",
      "                                      pdf_json_files  \\\n",
      "0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
      "1  document_parses/pdf_json/6b0567729c2143a66d737...   \n",
      "2  document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
      "3  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
      "4  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
      "\n",
      "                               pmc_json_files  \\\n",
      "0  document_parses/pmc_json/PMC35282.xml.json   \n",
      "1  document_parses/pmc_json/PMC59543.xml.json   \n",
      "2  document_parses/pmc_json/PMC59549.xml.json   \n",
      "3  document_parses/pmc_json/PMC59574.xml.json   \n",
      "4  document_parses/pmc_json/PMC59580.xml.json   \n",
      "\n",
      "                                                 url  s2_id  \n",
      "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n",
      "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
      "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
      "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
      "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n"
     ]
    }
   ],
   "source": [
    "# Quick exploration checks (Part 1)\n",
    "print('Columns:', df.columns.tolist())\n",
    "print('\\nMissing values (top 10 cols):')\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "print('\\nData types:')\n",
    "print(df.dtypes.value_counts())\n",
    "print('\\nFirst 5 rows:')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f783fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning shape: (1, 22)\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Cleaning\n",
    "def clean_metadata(df):\n",
    "    d = df.copy()\n",
    "    # publish_time -> datetime and year\n",
    "    if 'publish_time' in d.columns:\n",
    "        d['publish_time'] = pd.to_datetime(d['publish_time'], errors='coerce')\n",
    "        d['year'] = d['publish_time'].dt.year\n",
    "    else:\n",
    "        d['year'] = None\n",
    "    # title required for many analyses\n",
    "    if 'title' in d.columns:\n",
    "        d = d.dropna(subset=['title'])\n",
    "        d['title'] = d['title'].astype(str)\n",
    "        d['title_word_count'] = d['title'].str.split().str.len()\n",
    "    else:\n",
    "        d['title_word_count'] = 0\n",
    "    # abstract word count\n",
    "    if 'abstract' in d.columns:\n",
    "        d['abstract_word_count'] = d['abstract'].fillna('').astype(str).str.split().str.len()\n",
    "    else:\n",
    "        d['abstract_word_count'] = 0\n",
    "    # common fills\n",
    "    if 'journal' in d.columns:\n",
    "        d['journal'] = d['journal'].fillna('Unknown')\n",
    "    if 'source_x' in d.columns:\n",
    "        d['source_x'] = d['source_x'].fillna('Unknown')\n",
    "    # dedupe heuristics\n",
    "    if 's2_id' in d.columns:\n",
    "        d = d.drop_duplicates(subset=['s2_id'])\n",
    "    elif 'doi' in d.columns:\n",
    "        d = d.drop_duplicates(subset=['doi'])\n",
    "    else:\n",
    "        d = d.drop_duplicates(subset=['title'])\n",
    "    return d\n",
    "\n",
    "df_clean = clean_metadata(df)\n",
    "print('After cleaning shape:', df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b279a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cleaned csv to d:\\dataC transfer\\xampp\\htdocs\\python_frameworks_assignment\\metadata_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned file to project root\n",
    "out = Path.cwd() / 'metadata_cleaned.csv'\n",
    "df_clean.to_csv(out, index=False)\n",
    "print('Wrote cleaned csv to', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93abc3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote publications_by_year.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aondu\\AppData\\Local\\Temp\\ipykernel_2896\\2087595963.py:23: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=topj.values, y=topj.index, palette='mako', ax=ax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote top_journals.png\n",
      "Wrote n-gram CSVs\n",
      "Wrote wordcloud_improved.png\n",
      "Outputs written to d:\\dataC transfer\\xampp\\htdocs\\python_frameworks_assignment\\outputs\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Basic analysis and headless export (time series, top journals, n-grams, wordcloud)\n",
    "import os, re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "sns.set_style('whitegrid')\n",
    "outputs = Path.cwd() / 'outputs'\n",
    "outputs.mkdir(exist_ok=True)\n",
    "# publications by year\n",
    "if 'year' in df_clean.columns and df_clean['year'].notna().any():\n",
    "    yearly = df_clean.groupby('year').size().sort_index()\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.bar(yearly.index.astype(int), yearly.values, color='tab:blue')\n",
    "    ax.set_title('Publications by Year')\n",
    "    fig.savefig(outputs / 'publications_by_year.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print('Wrote publications_by_year.png')\n",
    "# top journals\n",
    "if 'journal' in df_clean.columns:\n",
    "    topj = df_clean['journal'].value_counts().head(20)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    sns.barplot(x=topj.values, y=topj.index, palette='mako', ax=ax)\n",
    "    fig.savefig(outputs / 'top_journals.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print('Wrote top_journals.png')\n",
    "# top unigrams/bigrams from titles\n",
    "stop = set(WordCloud().stopwords)\n",
    "def tokenize(s):\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', str(s).lower())\n",
    "    toks = [t for t in s.split() if t not in stop and len(t)>1]\n",
    "    return toks\n",
    "unis = Counter()\n",
    "bis = Counter()\n",
    "for t in df_clean['title'].dropna().astype(str):\n",
    "    toks = tokenize(t)\n",
    "    unis.update(toks)\n",
    "    bis.update([' '.join(x) for x in zip(toks, toks[1:])])\n",
    "top_unis = unis.most_common(30)\n",
    "top_bis = bis.most_common(30)\n",
    "import csv\n",
    "with open(outputs / 'top_unigrams.csv', 'w', newline='', encoding='utf-8') as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow(['unigram','count'])\n",
    "    writer.writerows(top_unis)\n",
    "with open(outputs / 'top_bigrams.csv', 'w', newline='', encoding='utf-8') as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow(['bigram','count'])\n",
    "    writer.writerows(top_bis)\n",
    "print('Wrote n-gram CSVs')\n",
    "# wordcloud image\n",
    "text = ' '.join(df_clean['title'].dropna().astype(str).tolist())\n",
    "wc = WordCloud(width=1200, height=600, background_color='white', stopwords=stop, max_words=200).generate(text)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "fig.savefig(outputs / 'wordcloud_improved.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "print('Wrote wordcloud_improved.png')\n",
    "print('Outputs written to', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac36dfa",
   "metadata": {},
   "source": [
    "## Part 4: Streamlit integration notes\n",
    "The Streamlit app `app.py` is provided at the repo root and reads the same `metadata_sample.csv` (or `metadata.csv`) used by this notebook. Run `streamlit run app.py` to view the interactive app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae9f81",
   "metadata": {},
   "source": [
    "## Part 5: Reflection\n",
    "Write a short reflection here about key findings and challenges (placeholder)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
